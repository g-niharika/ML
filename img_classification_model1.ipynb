{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SozrmFagYeBg",
    "outputId": "8b58983a-a18d-4fca-957a-43f6cbcb6fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "transform = torchvision.transforms.Compose( [torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xAiWA1bgZ_xl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_dimensions):\n",
    "        self.layer_dimensions = layer_dimensions\n",
    "        self.parameters = self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        parameters = {}\n",
    "        dim_length = len(self.layer_dimensions)\n",
    "        for layer_ind in range(1, dim_length):\n",
    "            distribution = np.sqrt(6 / (self.layer_dimensions[layer_ind] + self.layer_dimensions[layer_ind - 1]))\n",
    "            parameters[f'W{layer_ind}'] = np.random.uniform(-distribution ,distribution ,(self.layer_dimensions[layer_ind], self.layer_dimensions[layer_ind - 1]))\n",
    "            parameters[f'b{layer_ind}'] = np.zeros((self.layer_dimensions[layer_ind], 1))\n",
    "        return parameters\n",
    "\n",
    "    def affine_forward(self, A, W, b):\n",
    "        Z = np.dot(W, A) + b\n",
    "        aff_fwd_cache = (A, W, b)\n",
    "        return Z, aff_fwd_cache\n",
    "\n",
    "    def activation_forward(self, A):\n",
    "        Z = np.maximum(0, A)\n",
    "        atv_fwd_cache = A\n",
    "        return Z, atv_fwd_cache\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        fwd_ppg_caches = dict()\n",
    "        A = X\n",
    "        dim_length = len(self.layer_dimensions)\n",
    "        for layer_ind in range(1, dim_length):\n",
    "            Z, fwd_ppg_caches [f\"affine{layer_ind}\"] = self.affine_forward(A, self.parameters[f'W{layer_ind}'], self.parameters[f'b{layer_ind}'])\n",
    "            A, fwd_ppg_caches [f\"activation{layer_ind}\"] = self.activation_forward(Z)\n",
    "        return A, fwd_ppg_caches\n",
    "\n",
    "    def cost_function(self, AL, y):\n",
    "        # m = y.shape[1]\n",
    "        # cost = -(1/m) * np.sum(y * np.log(AL + 1e-15) + (1-y) * np.log(1 - AL + 1e-15))\n",
    "        # return cost\n",
    "        m = y.shape[1]\n",
    "        epsilon = 1e-15\n",
    "        AL = np.maximum(epsilon, AL)\n",
    "        AL = np.minimum(1 - epsilon, AL)\n",
    "        loss = -1/m * np.sum(y * np.log(AL) + (1 - y) * np.log(1 - AL))\n",
    "        return loss\n",
    "\n",
    "    def relu_backward(self, derivative_actvn, activation_cache):\n",
    "        Z = activation_cache\n",
    "        dZ = np.array(derivative_actvn, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "\n",
    "    def affine_backward(self, dZ, cache):\n",
    "        A_prev, W, b = cache\n",
    "        size = A_prev.shape[1]\n",
    "        derivative_Weight = np.dot(dZ, A_prev.T) / size\n",
    "        derivative_bias = np.sum(dZ, axis=1, keepdims=True) / size\n",
    "        derivative_actvn = np.dot(W.T, dZ)\n",
    "        return derivative_actvn, derivative_Weight, derivative_bias\n",
    "\n",
    "    def activation_backward(self, derivative_actvn, cache):\n",
    "        activation_cache = cache\n",
    "        dZ = self.relu_backward(derivative_actvn, activation_cache)\n",
    "        return dZ\n",
    "\n",
    "    def backward_propagation(self, lastlayer_opt, Y, caches):\n",
    "        gradients = {}\n",
    "        size = lastlayer_opt.shape[1]\n",
    "        derivative_actvn = lastlayer_opt\n",
    "        dim_length = len(self.layer_dimensions)\n",
    "        for layer_ind in reversed(range(1,dim_length )):\n",
    "          current_cache = caches[f'affine{layer_ind}']\n",
    "          derivative_actvn,gradients[f'dW{layer_ind}'],gradients[f'db{layer_ind}'] = self.affine_backward(derivative_actvn, current_cache)\n",
    "          if layer_ind>1:\n",
    "            current_cache = caches[f'activation{layer_ind-1}']\n",
    "            derivative_actvn = self.activation_backward(derivative_actvn, current_cache)\n",
    "        return gradients\n",
    "\n",
    "    def update_parameters(self, gradients, alpha):\n",
    "        dim_length = len(self.layer_dimensions)\n",
    "        for layer_ind in range(1, dim_length ):\n",
    "            self.parameters[f'W{layer_ind}'] -= alpha * gradients[f'dW{layer_ind}']\n",
    "            self.parameters[f'b{layer_ind}'] -= alpha * gradients[f'db{layer_ind}']\n",
    "\n",
    "    def train(self, X_train, X_val, y_train, y_val, epochs, alpha, batch_size):\n",
    "        size_xtrain = X_train.shape[1]\n",
    "        for epoch in range(epochs):\n",
    "            for x_point in range(0, size_xtrain, batch_size):\n",
    "                X_batch = X_train[:, x_point:x_point + batch_size]\n",
    "                y_batch = y_train[:, x_point:x_point + batch_size]\n",
    "                lastlayer_opt, caches = self.forward_propagation(X_batch)\n",
    "                cost = self.cost_function(lastlayer_opt, y_batch)\n",
    "                gradients = self.backward_propagation(lastlayer_opt - y_batch, y_batch, caches)\n",
    "                self.update_parameters(gradients, alpha)\n",
    "            train_predictions= self.predict(X_train)\n",
    "            val_predictions = self.predict(X_val)\n",
    "            train_accuracy = self.calculate_accuracy(train_predictions, y_train,X_train.shape[1])\n",
    "            val_accuracy = self.calculate_accuracy(val_predictions, y_val,X_val.shape[1])\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "            print(f\"  - Training Loss: {cost:.4f}\")\n",
    "            print(f\"  - Training Accuracy: {100*train_accuracy:.2f}%\")\n",
    "            print(f\"  - Validation Accuracy: {100*val_accuracy:.2f}%\")\n",
    "\n",
    "    def calculate_accuracy(self, lastlayer_opt, Y , size):\n",
    "        accuracy = np.sum(lastlayer_opt==np.argmax(Y,axis = 0)) / (size)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        lastlayer_opt, _ = self.forward_propagation(X_new)\n",
    "        return np.argmax(lastlayer_opt,axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XS_yDBzma-Wp",
    "outputId": "5a2fde1c-4501-4f1e-db40-20c0e60b2b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  - Training Loss: 2.9674\n",
      "  - Training Accuracy: 18.07%\n",
      "  - Validation Accuracy: 17.62%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "  img = img / 2 + 0.5\n",
    "  npimg = img.numpy()\n",
    "  plt.figure(figsize=(2,2))\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def changing_to_numpy(dataset):\n",
    "    x = np.array([np.array(x[0].numpy().reshape(3*32*32), dtype=np.float64) for x in dataset])\n",
    "    y = np.array([x[1] for x in dataset])\n",
    "    return x,y\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def custom_train_val_split(dataset, validation_size=0.1, random_seed=34):\n",
    "    np.random.seed(random_seed)\n",
    "    dataset_length = len(dataset)\n",
    "    indices = np.arange(dataset_length)\n",
    "    np.random.shuffle(indices)\n",
    "    validation_limit = int(validation_size * dataset_length)\n",
    "    validation_indices = indices[:validation_limit]\n",
    "    train_indices = indices[validation_limit:]\n",
    "    train_data = [dataset[i] for i in train_indices]\n",
    "    validation_data = [dataset[i] for i in validation_indices]\n",
    "    return train_data, validation_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layer_dimensions = [\n",
    "    3072,\n",
    "    1024,\n",
    "    512,\n",
    "    256,\n",
    "    128,\n",
    "    64,\n",
    "    32,\n",
    "    10\n",
    "]\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot_encoded = np.zeros((y.shape[0], num_classes))\n",
    "    for i_point in range(len(y)):\n",
    "        one_hot_encoded[i_point, y[i_point]] = 1\n",
    "    return one_hot_encoded\n",
    "\n",
    "\n",
    "\n",
    "nn_model = NeuralNetwork(layer_dimensions)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "train_data, val_data = custom_train_val_split(trainset)\n",
    "X_train, y_train = changing_to_numpy(train_data)\n",
    "X_val, y_val = changing_to_numpy(val_data)\n",
    "X_test,Y_test = changing_to_numpy(testset)\n",
    "\n",
    "y_train = one_hot_encode(y_train,10)\n",
    "y_val = one_hot_encode(y_val,10)\n",
    "y_test = one_hot_encode(Y_test,10)\n",
    "\n",
    "epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "nn_model.train(X_train.T, X_val.T, y_train.T, y_val.T, epochs, learning_rate, batch_size)\n",
    "p = nn_model.predict(X_test.T)\n",
    "testing_accuracy = nn_model.calculate_accuracy( p, y_test.T  , X_test.T.shape[1])\n",
    "testing_cost = nn_model.cost_function(p,y_test.T)\n",
    "\n",
    "print(f\"  - Testing Accuracy: {100*testing_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "  imshow(testset[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReBLBbNf8QMc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
